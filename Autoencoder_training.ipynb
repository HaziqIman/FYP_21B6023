{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b82de90-5722-4181-bbc7-670395667248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension after removing src/dst IPs: 104\n",
      "Epoch 1/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 611us/step - loss: 0.0191 - val_loss: 0.0168\n",
      "Epoch 2/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 575us/step - loss: 0.0172 - val_loss: 0.0166\n",
      "Epoch 3/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 560us/step - loss: 0.0171 - val_loss: 0.0166\n",
      "Epoch 4/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 561us/step - loss: 0.0170 - val_loss: 0.0166\n",
      "Epoch 5/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 561us/step - loss: 0.0169 - val_loss: 0.0165\n",
      "Epoch 6/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 564us/step - loss: 0.0168 - val_loss: 0.0163\n",
      "Epoch 7/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 565us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 8/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 571us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 9/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 558us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 10/10\n",
      "\u001b[1m250000/250000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 544us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Finished Training! and Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess data from a CSV file\n",
    "def preprocess_data_from_csv(file_path):\n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('smaller_update.csv', header=None, on_bad_lines='warn')\n",
    "\n",
    "    processed_data = []\n",
    "    max_length = 0\n",
    "\n",
    "    # First pass: find the maximum length of rows (number of hex values)\n",
    "    for row in data[0]:\n",
    "        row = row.replace('<head>', '').replace('<pkt>', '').replace('</s>', '').strip()\n",
    "        hex_values = row.split()\n",
    "        max_length = max(max_length, len(hex_values))\n",
    "    \n",
    "    # Second pass: convert hex values to integers, remove src/dst IP (if present), and pad\n",
    "    for row in data[0]:\n",
    "        row = row.replace('<head>', '').replace('<pkt>', '').replace('</s>', '').strip()\n",
    "        hex_values = row.split()\n",
    "        \n",
    "        # Convert hex values to integers\n",
    "        int_values = [int(x, 16) for x in hex_values]\n",
    "        \n",
    "        # Ignore source/destination IP addresses (bytes 12–19 in a standard IPv4 header),\n",
    "        # if the row is at least 20 bytes long\n",
    "        if len(int_values) >= 20:\n",
    "            del int_values[12:20]  # remove source and destination IP (8 bytes total)\n",
    "        \n",
    "        # Update the new max_length if needed\n",
    "        # (since we've potentially removed bytes, we might have a new max length)\n",
    "        max_length = max(max_length, len(int_values))\n",
    "        \n",
    "        processed_data.append(int_values)\n",
    "    \n",
    "    # Pad rows with zeros to ensure uniform length after IP removal\n",
    "    for i in range(len(processed_data)):\n",
    "        if len(processed_data[i]) < max_length:\n",
    "            processed_data[i].extend([0] * (max_length - len(processed_data[i])))\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    processed_data = np.array(processed_data)\n",
    "    \n",
    "    # Normalize the data between 0 and 1 using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    processed_data = scaler.fit_transform(processed_data)\n",
    "\n",
    "    return processed_data, scaler\n",
    "\n",
    "# Define Autoencoder Model\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    \n",
    "    # Latent representation (code layer)\n",
    "    encoded = Dense(16, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Train Autoencoder\n",
    "def train_autoencoder(autoencoder, data):\n",
    "    # Using a slightly more robust early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        data, data,\n",
    "        epochs=10,           # Increased epochs for better training\n",
    "        batch_size=64,       # You can adjust batch size based on GPU/CPU memory\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Save Autoencoder Model\n",
    "def save_autoencoder(autoencoder, filename='newest_model.keras'):\n",
    "    autoencoder.save(filename)\n",
    "\n",
    "# Load Autoencoder Model\n",
    "def load_autoencoder(filename='newest_model.keras'):\n",
    "    return tf.keras.models.load_model(filename)\n",
    "\n",
    "\n",
    "# ------------------\n",
    "# Example usage\n",
    "# ------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # CSV file path containing the training data\n",
    "    csv_file_path = 'smaller_update.csv'\n",
    "\n",
    "    # Step 1: Load and preprocess the training dataset\n",
    "    processed_data, scaler = preprocess_data_from_csv(csv_file_path)\n",
    "\n",
    "    # Step 2: Build the autoencoder model\n",
    "    input_dim = processed_data.shape[1]  # Number of features based on the CSV file\n",
    "    print(\"Input dimension after removing src/dst IPs:\", input_dim)\n",
    "    \n",
    "    autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "    # Step 3: Train the autoencoder\n",
    "    history = train_autoencoder(autoencoder, processed_data)\n",
    "\n",
    "    # Step 4: Save the autoencoder\n",
    "    save_autoencoder(autoencoder, filename='newest_model.keras')\n",
    "    print(\"Finished Training! and Saved\")\n",
    "\n",
    "    # In this improved version, we do not test on a single packet.\n",
    "    # Any anomaly detection or evaluation will happen separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882b1ec-b474-4adc-adae-52d180fa974d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
